{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sentiment analysis</h1>\n",
    "<p>By Turlagh CLANCY (20220024) and Beno√Æt LU (20141188)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We first import the basic library that will be used everywhere</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The dataset classes will help us iterate through the data and split it.</p>\n",
    "<p>The DFDataset is just a container for the train/test splits from the original dataset.</p>\n",
    "<p>SSTDataset is a container for the stanford's dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DFDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.df[['phrases', 'sentiment values']].iloc[idx].to_dict()\n",
    "    \n",
    "    def splits(self, test_size=0.3):\n",
    "        train_df, test_df = train_test_split(self.df, test_size=test_size)\n",
    "        return DFDataset(train_df), DFDataset(test_df)\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, data_folder):\n",
    "        self.data_folder = data_folder\n",
    "        df_sent = pd.read_table(f'{data_folder}/datasetSentences.txt')\n",
    "        df_lab = pd.read_table(f'{data_folder}/sentiment_labels.txt',sep='|')\n",
    "        df_dict = pd.read_table(f'{data_folder}/dictionary.txt',sep='|', names=('phrases','phrase ids'))\n",
    "        df_common = df_dict[df_dict['phrases'].isin(df_sent['sentence'])]\n",
    "        self.df = df_lab.join(df_common.set_index('phrase ids'), on='phrase ids', how='right')\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.df[['phrases', 'sentiment values']].iloc[idx].to_dict()\n",
    "    \n",
    "    def splits(self, test_size=0.3):\n",
    "        train_df, test_df = train_test_split(self.df, test_size=test_size)\n",
    "        return DFDataset(train_df), DFDataset(test_df)\n",
    "    \n",
    "    def phrases(self):\n",
    "        return self.df['phrases']\n",
    "\n",
    "SST = SSTDataset('./data')\n",
    "train_dataset, test_dataset = SST.splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We're now loading each embedding that'll be used throughout the project <b>beware of their size</b>, they will be loaded at the root of the project.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe pre-train model\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(name='6B', dim=100, unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec pre-train model\n",
    "import os\n",
    "os.environ[\"GENSIM_DATA_DIR\"] = f\"D:\\python projects\\sentiment-analysis\\gensim-data\"\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText pre-train model\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "fasttext = FastText(language='en', unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "documents = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(SST.phrases())]\n",
    "doc2vec = Doc2Vec(documents, vector_size=300, window=10, workers=4, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We sample from the dataset in batch-mode, we have to make sure that each batch contains targets of equal length, else, we pad them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "def collate_batch(batch, text_transform_fn, pad_id):\n",
    "    labels, texts, lengths = [], [], []\n",
    "    for row in batch:\n",
    "        labels.append(row['sentiment values'])\n",
    "        tokens = text_transform_fn(row['phrases'])\n",
    "        texts.append(torch.tensor(tokens))\n",
    "        lengths.append(len(tokens))\n",
    "    # Decreasing sorting required for packed padded sequences\n",
    "    df = pd.DataFrame({'texts':texts, 'lengths':lengths}).sort_values(by='lengths', ascending=False)\n",
    "    return torch.tensor(labels), pad_sequence(df['texts'].tolist(), padding_value=pad_id), torch.tensor(df['lengths'].tolist())\n",
    "\n",
    "# src: https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb\n",
    "def batch_sampler(dataset, tokenizer, batch_size):\n",
    "    indices = [(i, len(tokenizer(s['phrases']))) for i, s in enumerate(dataset)]\n",
    "    random.shuffle(indices)\n",
    "    pooled_indices = []\n",
    "    # create pool of indices with similar lengths \n",
    "    for i in range(0, len(indices), batch_size * 100):\n",
    "        pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
    "\n",
    "    pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "    # yield indices for current batch\n",
    "    return [pooled_indices[i:i + batch_size] for i in range(0, len(pooled_indices), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here are the neural networks classes. Both GRU and LSTM share the same architecture aside from the core of the network which is LSTM or GRU. They're redundant in their definition, but it's clearer to explicitly instantiate an LSTM or a GRU, instead of passing a parameter (\"lstm\", \"gru\")/p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx, freeze_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx = pad_idx, freeze=freeze_embeddings)\n",
    "        self.rnn = nn.LSTM(pretrained_embeddings.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, lengths):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.to('cpu'))\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx, freeze_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx = pad_idx, freeze=freeze_embeddings)\n",
    "        self.rnn = nn.GRU(pretrained_embeddings.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, lengths):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.to('cpu'))\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "            \n",
    "        return self.fc(hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here are first the parameters that will be passed to the neural network and second we have a generic setup function that takes into account the architecture and the embeddings to use, as per the project statement we're going through LSTM or GRU for the architecture and GlOve, FastText, Word2Vec and document-level for the embeddings.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "FREEZE_EMBEDDINGS = True\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def setup(architecture, embeddings):\n",
    "    if embeddings.lower() == 'glove':\n",
    "        voc = glove.stoi\n",
    "        pretrain_emb = glove.vectors\n",
    "    elif embeddings.lower() == 'fasttext':\n",
    "        voc = fasttext.stoi\n",
    "        pretrain_emb = fasttext.vectors\n",
    "    elif embeddings.lower() == 'word2vec':\n",
    "        voc = word2vec.key_to_index\n",
    "        pretrain_emb = torch.from_numpy(word2vec.vectors)\n",
    "    elif embeddings.lower() == 'documentlevel':\n",
    "        voc = doc2vec.wv.key_to_index\n",
    "        pretrain_emb = torch.from_numpy(doc2vec.wv.vectors)\n",
    "    else:\n",
    "        print(\"Unknown embeddings passed, failed to setup\")\n",
    "        return None\n",
    "    \n",
    "    specials = {'<unk>':len(voc), '<pad>':len(voc) + 1}\n",
    "    tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "    batch_size = 8\n",
    "    text_transform = lambda x: [voc[token] if token in voc else specials['<unk>'] for token in tokenizer(x)]\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_sampler=batch_sampler(train_dataset, tokenizer, batch_size),\n",
    "                                  collate_fn=lambda batch: collate_batch(batch, text_transform, specials['<pad>']))\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_sampler=batch_sampler(test_dataset, tokenizer, batch_size),\n",
    "                                 collate_fn=lambda batch: collate_batch(batch, text_transform, specials['<pad>']))\n",
    "\n",
    "    # We concatenate the special characters\n",
    "    pretrain_emb = torch.cat((pretrain_emb, torch.zeros(len(specials),pretrain_emb.shape[1])))\n",
    "    \n",
    "    if architecture.lower() == 'lstm':\n",
    "        model = LSTM(pretrain_emb, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, specials['<pad>'], FREEZE_EMBEDDINGS)\n",
    "    elif architecture.lower() == 'gru':\n",
    "        model = GRU(pretrain_emb, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, specials['<pad>'], FREEZE_EMBEDDINGS)\n",
    "    else:\n",
    "        print(\"Unknown architecture passed, failed to setup\")\n",
    "        return None\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "    return (model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We train and evaluate the model though each datalaoder (which iterates through the dataset in batch-mode) using an accuracy based on the 5 classes of the stanford dataset, from 0 to 1 by a step of 0.2.</p>\n",
    "<p>The train and evaluate functions are the same, aside from the optimization through the gradient being enabled during training.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import time\n",
    "\n",
    "def accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.floor(5*torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == torch.floor(5*y)).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        labels, inputs, inputs_lengths = batch\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs, inputs_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            labels, inputs, inputs_lengths = batch\n",
    "\n",
    "            predictions = model(inputs, inputs_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Each pair of architecture and embedding is improved upon through <i>N_EPOCHS</i> epochs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "N_EPOCHS = 8\n",
    "\n",
    "def compute_epochs(model, train_dataloader, test_dataloader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    test_accs = []\n",
    "    train_accs = []\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "        test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')\n",
    "        test_accs.append(test_acc)\n",
    "        train_accs.append(train_acc)\n",
    "    return train_accs, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We generate the corresponding accuracies here.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "EPOCHS ON LSTM with GlOve\n",
      "==========================\n",
      "The model has 121,701 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 44s\n",
      "\tTrain Loss: 0.688 | Train Acc: 21.43%\n",
      "\t Test Loss: 0.680 |  Test Acc: 21.76%\n",
      "Epoch: 02 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.679 | Train Acc: 24.09%\n",
      "\t Test Loss: 0.679 |  Test Acc: 25.56%\n",
      "Epoch: 03 | Epoch Time: 1m 3s\n",
      "\tTrain Loss: 0.676 | Train Acc: 24.44%\n",
      "\t Test Loss: 0.675 |  Test Acc: 26.48%\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.676 | Train Acc: 25.30%\n",
      "\t Test Loss: 0.678 |  Test Acc: 26.33%\n",
      "Epoch: 05 | Epoch Time: 1m 3s\n",
      "\tTrain Loss: 0.674 | Train Acc: 26.10%\n",
      "\t Test Loss: 0.674 |  Test Acc: 28.05%\n",
      "Epoch: 06 | Epoch Time: 1m 0s\n",
      "\tTrain Loss: 0.674 | Train Acc: 26.25%\n",
      "\t Test Loss: 0.674 |  Test Acc: 27.96%\n",
      "Epoch: 07 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.673 | Train Acc: 26.30%\n",
      "\t Test Loss: 0.673 |  Test Acc: 27.14%\n",
      "Epoch: 08 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.672 | Train Acc: 26.68%\n",
      "\t Test Loss: 0.676 |  Test Acc: 26.10%\n",
      "==========================\n",
      "EPOCHS ON LSTM with Fasttext\n",
      "==========================\n",
      "The model has 201,701 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.684 | Train Acc: 21.99%\n",
      "\t Test Loss: 0.675 |  Test Acc: 24.97%\n",
      "Epoch: 02 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.674 | Train Acc: 25.68%\n",
      "\t Test Loss: 0.671 |  Test Acc: 25.47%\n",
      "Epoch: 03 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.671 | Train Acc: 26.71%\n",
      "\t Test Loss: 0.672 |  Test Acc: 26.36%\n",
      "Epoch: 04 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.668 | Train Acc: 28.80%\n",
      "\t Test Loss: 0.671 |  Test Acc: 27.45%\n",
      "Epoch: 05 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.667 | Train Acc: 29.12%\n",
      "\t Test Loss: 0.672 |  Test Acc: 27.07%\n",
      "Epoch: 06 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.665 | Train Acc: 29.33%\n",
      "\t Test Loss: 0.672 |  Test Acc: 26.36%\n",
      "Epoch: 07 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.663 | Train Acc: 30.15%\n",
      "\t Test Loss: 0.672 |  Test Acc: 27.63%\n",
      "Epoch: 08 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.661 | Train Acc: 30.71%\n",
      "\t Test Loss: 0.672 |  Test Acc: 27.78%\n",
      "==========================\n",
      "EPOCHS ON LSTM with word2vec\n",
      "==========================\n",
      "The model has 201,701 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.680 | Train Acc: 23.68%\n",
      "\t Test Loss: 0.675 |  Test Acc: 23.93%\n",
      "Epoch: 02 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.671 | Train Acc: 28.06%\n",
      "\t Test Loss: 0.672 |  Test Acc: 26.35%\n",
      "Epoch: 03 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.668 | Train Acc: 28.88%\n",
      "\t Test Loss: 0.671 |  Test Acc: 27.12%\n",
      "Epoch: 04 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.666 | Train Acc: 28.93%\n",
      "\t Test Loss: 0.671 |  Test Acc: 28.06%\n",
      "Epoch: 05 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.664 | Train Acc: 30.18%\n",
      "\t Test Loss: 0.671 |  Test Acc: 28.09%\n",
      "Epoch: 06 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.662 | Train Acc: 30.56%\n",
      "\t Test Loss: 0.672 |  Test Acc: 27.59%\n",
      "Epoch: 07 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.659 | Train Acc: 31.24%\n",
      "\t Test Loss: 0.672 |  Test Acc: 27.35%\n",
      "Epoch: 08 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.659 | Train Acc: 31.05%\n",
      "\t Test Loss: 0.672 |  Test Acc: 27.77%\n",
      "==========================\n",
      "EPOCHS ON LSTM with documentlevel\n",
      "==========================\n",
      "The model has 201,701 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.688 | Train Acc: 21.01%\n",
      "\t Test Loss: 0.683 |  Test Acc: 23.20%\n",
      "Epoch: 02 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.682 | Train Acc: 23.01%\n",
      "\t Test Loss: 0.681 |  Test Acc: 23.73%\n",
      "Epoch: 03 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.679 | Train Acc: 23.87%\n",
      "\t Test Loss: 0.681 |  Test Acc: 26.30%\n",
      "Epoch: 04 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.677 | Train Acc: 25.14%\n",
      "\t Test Loss: 0.681 |  Test Acc: 25.74%\n",
      "Epoch: 05 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.675 | Train Acc: 26.52%\n",
      "\t Test Loss: 0.681 |  Test Acc: 25.83%\n",
      "Epoch: 06 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.673 | Train Acc: 26.81%\n",
      "\t Test Loss: 0.683 |  Test Acc: 27.10%\n",
      "Epoch: 07 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.671 | Train Acc: 27.19%\n",
      "\t Test Loss: 0.682 |  Test Acc: 25.98%\n",
      "Epoch: 08 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.668 | Train Acc: 28.00%\n",
      "\t Test Loss: 0.685 |  Test Acc: 27.07%\n",
      "==========================\n",
      "EPOCHS ON GRU with GlOve\n",
      "==========================\n",
      "The model has 91,301 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.689 | Train Acc: 20.82%\n",
      "\t Test Loss: 0.678 |  Test Acc: 22.76%\n",
      "Epoch: 02 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.680 | Train Acc: 24.10%\n",
      "\t Test Loss: 0.677 |  Test Acc: 24.51%\n",
      "Epoch: 03 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.678 | Train Acc: 24.90%\n",
      "\t Test Loss: 0.678 |  Test Acc: 24.54%\n",
      "Epoch: 04 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.676 | Train Acc: 25.66%\n",
      "\t Test Loss: 0.675 |  Test Acc: 26.37%\n",
      "Epoch: 05 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.675 | Train Acc: 25.91%\n",
      "\t Test Loss: 0.676 |  Test Acc: 26.46%\n",
      "Epoch: 06 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.675 | Train Acc: 26.29%\n",
      "\t Test Loss: 0.678 |  Test Acc: 25.35%\n",
      "Epoch: 07 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.674 | Train Acc: 27.24%\n",
      "\t Test Loss: 0.676 |  Test Acc: 25.83%\n",
      "Epoch: 08 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.672 | Train Acc: 27.37%\n",
      "\t Test Loss: 0.677 |  Test Acc: 26.89%\n",
      "==========================\n",
      "EPOCHS ON GRU with Fasttext\n",
      "==========================\n",
      "The model has 151,301 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.686 | Train Acc: 21.59%\n",
      "\t Test Loss: 0.676 |  Test Acc: 23.27%\n",
      "Epoch: 02 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.678 | Train Acc: 24.53%\n",
      "\t Test Loss: 0.674 |  Test Acc: 22.53%\n",
      "Epoch: 03 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.673 | Train Acc: 25.92%\n",
      "\t Test Loss: 0.672 |  Test Acc: 26.10%\n",
      "Epoch: 04 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.672 | Train Acc: 26.90%\n",
      "\t Test Loss: 0.671 |  Test Acc: 26.49%\n",
      "Epoch: 05 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.670 | Train Acc: 27.91%\n",
      "\t Test Loss: 0.673 |  Test Acc: 28.73%\n",
      "Epoch: 06 | Epoch Time: 0m 44s\n",
      "\tTrain Loss: 0.669 | Train Acc: 27.70%\n",
      "\t Test Loss: 0.673 |  Test Acc: 27.52%\n",
      "Epoch: 07 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.665 | Train Acc: 29.38%\n",
      "\t Test Loss: 0.677 |  Test Acc: 28.88%\n",
      "Epoch: 08 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.664 | Train Acc: 29.85%\n",
      "\t Test Loss: 0.675 |  Test Acc: 27.61%\n",
      "==========================\n",
      "EPOCHS ON GRU with word2vec\n",
      "==========================\n",
      "The model has 151,301 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.680 | Train Acc: 24.27%\n",
      "\t Test Loss: 0.672 |  Test Acc: 25.57%\n",
      "Epoch: 02 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.669 | Train Acc: 27.77%\n",
      "\t Test Loss: 0.671 |  Test Acc: 27.31%\n",
      "Epoch: 03 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.667 | Train Acc: 29.03%\n",
      "\t Test Loss: 0.670 |  Test Acc: 28.24%\n",
      "Epoch: 04 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.664 | Train Acc: 29.60%\n",
      "\t Test Loss: 0.671 |  Test Acc: 28.82%\n",
      "Epoch: 05 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.664 | Train Acc: 29.33%\n",
      "\t Test Loss: 0.672 |  Test Acc: 30.31%\n",
      "Epoch: 06 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.661 | Train Acc: 30.60%\n",
      "\t Test Loss: 0.673 |  Test Acc: 30.38%\n",
      "Epoch: 07 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.659 | Train Acc: 31.45%\n",
      "\t Test Loss: 0.673 |  Test Acc: 31.71%\n",
      "Epoch: 08 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.656 | Train Acc: 32.74%\n",
      "\t Test Loss: 0.676 |  Test Acc: 31.85%\n",
      "==========================\n",
      "EPOCHS ON GRU with documentlevel\n",
      "==========================\n",
      "The model has 151,301 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 44s\n",
      "\tTrain Loss: 0.690 | Train Acc: 21.61%\n",
      "\t Test Loss: 0.685 |  Test Acc: 24.89%\n",
      "Epoch: 02 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.684 | Train Acc: 22.71%\n",
      "\t Test Loss: 0.683 |  Test Acc: 24.06%\n",
      "Epoch: 03 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.681 | Train Acc: 24.20%\n",
      "\t Test Loss: 0.682 |  Test Acc: 25.45%\n",
      "Epoch: 04 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.679 | Train Acc: 24.99%\n",
      "\t Test Loss: 0.682 |  Test Acc: 26.58%\n",
      "Epoch: 05 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.678 | Train Acc: 25.85%\n",
      "\t Test Loss: 0.682 |  Test Acc: 26.55%\n",
      "Epoch: 06 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.676 | Train Acc: 26.74%\n",
      "\t Test Loss: 0.682 |  Test Acc: 26.40%\n",
      "Epoch: 07 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.674 | Train Acc: 27.10%\n",
      "\t Test Loss: 0.683 |  Test Acc: 27.08%\n",
      "Epoch: 08 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.672 | Train Acc: 27.37%\n",
      "\t Test Loss: 0.684 |  Test Acc: 27.20%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "arrays = [\n",
    "    ['lstm', 'lstm', 'lstm', 'lstm', 'gru', 'gru', 'gru', 'gru'],\n",
    "    ['glove', 'fasttext', 'word2vec', 'documentlevel', 'glove', 'fasttext', 'word2vec', 'documentlevel']\n",
    "]\n",
    "tuples = list(zip(*arrays))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"architecture\", \"embedding\"])\n",
    "df = pd.DataFrame([], index=index, columns=['train_acc', 'test_acc'])\n",
    "\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON LSTM with GlOve\")\n",
    "print(\"==========================\")\n",
    "df.loc['lstm', 'glove'][['train_acc', 'test_acc']] = compute_epochs(*setup('lstm','glove'))\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON LSTM with Fasttext\")\n",
    "print(\"==========================\")\n",
    "df.loc['lstm', 'fasttext'][['train_acc', 'test_acc']] = compute_epochs(*setup('lstm','fasttext'))\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON LSTM with word2vec\")\n",
    "print(\"==========================\")\n",
    "df.loc['lstm', 'word2vec'][['train_acc', 'test_acc']] = compute_epochs(*setup('lstm','word2vec'))\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON LSTM with documentlevel\")\n",
    "print(\"==========================\")\n",
    "df.loc['lstm', 'documentlevel'][['train_acc', 'test_acc']] = compute_epochs(*setup('lstm','documentlevel'))\n",
    "\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON GRU with GlOve\")\n",
    "print(\"==========================\")\n",
    "df.loc['gru', 'glove'][['train_acc', 'test_acc']] = compute_epochs(*setup('gru','glove'))\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON GRU with Fasttext\")\n",
    "print(\"==========================\")\n",
    "df.loc['gru', 'fasttext'][['train_acc', 'test_acc']] = compute_epochs(*setup('gru','fasttext'))\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON GRU with word2vec\")\n",
    "print(\"==========================\")\n",
    "df.loc['gru', 'word2vec'][['train_acc', 'test_acc']] = compute_epochs(*setup('gru','word2vec'))\n",
    "print(\"==========================\")\n",
    "print(\"EPOCHS ON GRU with documentlevel\")\n",
    "print(\"==========================\")\n",
    "df.loc['gru', 'documentlevel'][['train_acc', 'test_acc']] = compute_epochs(*setup('gru','documentlevel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "params_tuples = [('lstm', 'glove', 'LSTM (GlOve)'),\n",
    "          ('lstm', 'fasttext', 'LSTM (FastText)'),\n",
    "          ('lstm', 'word2vec', 'LSTM (Word2Vec Google News 300)'),\n",
    "          ('lstm', 'documentlevel', 'LSTM (document-level)'),\n",
    "          ('gru', 'glove', 'GRU (GlOve)'),\n",
    "          ('gru', 'fasttext', 'GRU (FastText)'),\n",
    "          ('gru', 'word2vec', 'GRU (Word2Vec Google News 300)'),\n",
    "          ('gru', 'documentlevel', 'GRU (document-level)'),]\n",
    "\n",
    "for params in params_tuples:\n",
    "    plt.plot(np.arange(1,9), df.loc[params[0], params[1]]['train_acc'], label='train', linewidth=3)\n",
    "    plt.plot(np.arange(1,9), df.loc[params[0], params[1]]['test_acc'], label='test', linewidth=3)\n",
    "    plt.title(params[2], fontsize=25)\n",
    "    plt.xlabel('epoch', fontsize=20)\n",
    "    plt.ylabel('accuracy', fontsize=20)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.ylim(0.2,0.35)\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.savefig(f'{params[0]}_{params[1]}.png', bbox_inches='tight')\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "./.venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
